{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onnx vs PyTorch performance example\n",
    "\n",
    "This example compares inference time for denoising models defined in the\n",
    "`DUNEdn` package.\n",
    "\n",
    "The models implemented in PyTorch are exported to Onnx format and both are used\n",
    "to make inference separately.\n",
    "\n",
    "The elapsed time for PyTorch and Onnx models batch prediction is measured for\n",
    "different batch sizes.\n",
    "\n",
    "The image below shows the pipeline adopted for the current example. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"assets/performance.png\" alt=\"Onnx performance example\" width=60%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from assets.functions import (\n",
    "    prepare_folders_and_paths,\n",
    "    check_in_output_folder,\n",
    "    compare_performance_onnx,\n",
    "    plot_comparison_catplot,\n",
    ")\n",
    "from dunedn.inference.hitreco import DnModel\n",
    "from dunedn.utils.utils import load_runcard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define user inputs.\n",
    "\n",
    "The user might want to tweak the following variables to experiment with the `DUNEdn` package.\n",
    "\n",
    "- **modeltype** -> available options: `cnn`, `gcnn`, `uscg`.\n",
    "- **version**  -> available options: `v08`, `v09`  \n",
    "  The dataset version where the model was trained on.  \n",
    "  For `cnn` and `gcnn` networks, only version `v08` is available.\n",
    "- **pytorch_dev** -> available options: `cpu`, `cuda:0` or `cuda:id`.  \n",
    "  The device hosting the PyTorch computation.  \n",
    "  It is recommended to run PyTorch on a GPU.  \n",
    "  Default ``batch_size`` settings ensure that the computation fits a 16 GB gpu.  \n",
    "- **base_folder** -> the output folder.  \n",
    "  Ensure to have permissions to write on the device.\n",
    "- **ckpt_folder** -> the checkpoint folder.  \n",
    "  Ensure the folder has the structure explained in the package documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user inputs\n",
    "modeltype = \"cnn\"\n",
    "version = \"v08\"\n",
    "pytorch_dev = \"cpu\"  # device hosting PyTorch computation\n",
    "base_folder = Path(\"../../output/tmp\")\n",
    "ckpt_folder = Path(f\"../dunedn_checkpoints/{modeltype}_{version}\")\n",
    "\n",
    "# set up the environment\n",
    "folders, paths = prepare_folders_and_paths(modeltype, version, base_folder, ckpt_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_output_folder(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model loading: PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = load_runcard(base_folder / \"cards/runcard.yaml\")  # settings\n",
    "model = DnModel(setup, modeltype, ckpt_folder)\n",
    "print(f\"Loaded model from {ckpt_folder} folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model loading: Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "model.onnx_export(folders[\"onnx_save\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_onnx = DnModel(setup, modeltype, folders[\"onnx_save\"], should_use_onnx=True)\n",
    "print(f\"Loaded model from {folders['onnx_save']} folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch vs Onnx Performance\n",
    "\n",
    "The goal is to compare the performance of the models for different input batch sizes.\n",
    "\n",
    "The collected inference timings are loaded into a `Pandas.Dataframe` for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [32, 64, 128, 256, 512, 1024]\n",
    "nb_batches = 2\n",
    "performance_df = compare_performance_onnx(\n",
    "    model, model_onnx, pytorch_dev, batch_size_list, nb_batches\n",
    ")\n",
    "performance_df.to_csv(paths[\"performance_csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.read_csv(paths[\"performance_csv\"])\n",
    "plot_comparison_catplot(performance_df, folders[\"plot\"], with_graphics=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e80351b5c4fff56b32463330d4c3134db2ccc1cad158f50be317425bad0b8b08"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
