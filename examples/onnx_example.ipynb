{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time as tm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from dunedn.inference.hitreco import DnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltype = \"cnn\"\n",
    "version = \"v08\"\n",
    "outdir = Path(\"../benchmarks/onnx/onnx_benchmark\")\n",
    "ckpt = Path(f\"../saved_models/{modeltype}_{version}\") # folder with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded event at ../benchmarks/onnx/onnx_benchmark/p2GeV_cosmics_inspired_rawdigit_evt8.npy\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "\n",
    "# print(f\"Extracting data into {outdir}...\")\n",
    "# !mkdir -p $outdir\n",
    "# tar -xf dunetpc_inspired_v09_p2GeV_rawdigits.tar.gz -C $outdir\n",
    "\n",
    "fname = outdir / \"p2GeV_cosmics_inspired_rawdigit_evt8.npy\"\n",
    "evt = np.load(fname)\n",
    "print(f\"Loaded event at {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference function\n",
    "def inference(model, evt, fname):\n",
    "    \"\"\"Makes inference on event and computes time.\n",
    "\n",
    "    Saves the output file to `fname`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: DnModel\n",
    "        The pytorch or onnx based model.\n",
    "    evt: np.ndarray\n",
    "        The input raw data.\n",
    "    fname: Path\n",
    "        The output file name.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    inference_time: float\n",
    "        The elapsed time for inference.    \n",
    "    \"\"\"\n",
    "    start = tm()\n",
    "    evt_dn = model.predict(evt)\n",
    "    inference_time = tm() - start\n",
    "\n",
    "    # save pytorch inference\n",
    "    np.save(fname, evt_dn)\n",
    "    return inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ../saved_models/gcnn_v08 folder\n"
     ]
    }
   ],
   "source": [
    "# PyTorch model loading\n",
    "model = DnModel(modeltype, ckpt)\n",
    "print(f\"Loaded model from {ckpt} folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch inference\n",
    "fname = outdir / \"pytorch_inference_results.npy\"\n",
    "pytorch_time = inference(model, evt, fname)\n",
    "print(f\"PyTorch inference done in {pytorch_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/anaconda3/envs/py39/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:55: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\"Specified provider '{}' is not in available provider names.\"\n"
     ]
    }
   ],
   "source": [
    "# uncomment this line to export model to ONNX format\n",
    "# model.export_onnx(outdir / f\"saved_models/{modeltype}_{version}\")\n",
    "\n",
    "model_onnx = DnModel(\n",
    "    modeltype,\n",
    "    outdir / f\"saved_models/{modeltype}_{version}\",\n",
    "    should_use_onnx=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/156 [00:49<2:09:01, 49.95s/it]"
     ]
    }
   ],
   "source": [
    "# ONNX inference\n",
    "fname = outdir / \"onnx_inference_results.npy\"\n",
    "onnx_time = inference(model_onnx, evt, fname)\n",
    "print(f\"PyTorch inference done in {onnx_time}s\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "911d4ec775e09918bcbd01389c7f006ca7306440d9fd811735413dee60452da2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
