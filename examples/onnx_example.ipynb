{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onnx vs PyTorch example\n",
    "\n",
    "This example loads an event inspired by ProtoDUNE-SP simulated data and puts it\n",
    "through the `DUNEdn` denoising pipeline.\n",
    "\n",
    "The models implemented in PyTorch are exported to Onnx format and both are used\n",
    "to make inference separately.\n",
    "\n",
    "The outputs are then exploited to make accuracy and performance comparisons.\n",
    "\n",
    "- **Accuracy**  \n",
    "  Denoised events are analysed against ground truth labels from Monte\n",
    "  Carlo simulation.  \n",
    "  Four different metrics (namely `mse`, `psnr`, `ssim` and `iMAE`) are evaluated\n",
    "  as in the [paper](https://doi.org/10.1007/s41781-021-00077-9).\n",
    "- **Performance**  \n",
    "  The elapsed time for PyTorch and Onnx models batch prediction is measured for\n",
    "  different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from plot_event_example import plot_example\n",
    "from assets.functions import check_in_output_folder, inference\n",
    "from dunedn.inference.hitreco import DnModel\n",
    "from dunedn.inference.analysis import analysis_main\n",
    "from dunedn.utils.utils import load_runcard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltype = \"uscg\"\n",
    "version = \"v08\"\n",
    "pytorch_dev = \"cpu\"  # device hosting PyTorch computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base folders\n",
    "base_folder = Path(\"../../output/tmp\")\n",
    "ckpt_folder = Path(f\"../saved_models/{modeltype}_{version}\")\n",
    "\n",
    "# relative folders\n",
    "folders = {\n",
    "    \"base\": base_folder,\n",
    "    \"out\": base_folder / \"models/onnx\",\n",
    "    \"ckpt\": ckpt_folder,\n",
    "    \"cards\": base_folder / f\"cards\",\n",
    "    \"onnx_save\": base_folder / f\"models/onnx/saved_models/{modeltype}_{version}\",\n",
    "    \"plot\": base_folder / \"models/onnx/plots\",\n",
    "    \"id_plot\": base_folder / \"models/onnx/plots/identity\",\n",
    "    \"pytorch_plot\": base_folder / \"models/onnx/plots/torch\",\n",
    "    \"onnx_plot\": base_folder / \"models/onnx/plots/onnx\",\n",
    "}\n",
    "\n",
    "check_in_output_folder(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to files\n",
    "paths = {\n",
    "    \"input\": folders[\"out\"] / \"p2GeV_cosmics_inspired_rawdigit_evt8.npy\",\n",
    "    \"target\": folders[\"out\"] / \"p2GeV_cosmics_inspired_rawdigit_noiseoff_evt8.npy\",\n",
    "    \"pytorch\": folders[\"out\"]\n",
    "    / f\"p2GeV_cosmics_inspired_rawdigit_torch_{modeltype}_evt8.npy\",\n",
    "    \"onnx\": folders[\"out\"]\n",
    "    / f\"p2GeV_cosmics_inspired_rawdigit_onnx_{modeltype}_evt8.npy\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example(\n",
    "    paths[\"input\"], paths[\"target\"], outdir=folders[\"id_plot\"], with_graphics=True\n",
    ")\n",
    "\n",
    "evt = np.load(paths[\"input\"])[:, 2:]\n",
    "print(f\"Loaded event at {paths['input']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch inference\n",
    "\n",
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = load_runcard(base_folder / \"cards/runcard.yaml\")  # settings\n",
    "model = DnModel(setup, modeltype, ckpt_folder)\n",
    "print(f\"Loaded model from {ckpt_folder} folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_time = inference(model, evt, paths[\"pytorch\"], pytorch_dev)\n",
    "print(f\"PyTorch inference done in {pytorch_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: accuracy assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "analysis_main(paths[\"pytorch\"], paths[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plot\n",
    "plot_example(\n",
    "    paths[\"pytorch\"],\n",
    "    paths[\"target\"],\n",
    "    outdir=folders[\"pytorch_plot\"],\n",
    "    with_graphics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx inference\n",
    "\n",
    "### Export to and load model from Onnx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "model.onnx_export(folders[\"onnx_save\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_onnx = DnModel(setup, modeltype, folders[\"onnx_save\"], should_use_onnx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onnx inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX inference\n",
    "onnx_time = inference(model_onnx, evt, paths[\"onnx\"])\n",
    "print(f\"ONNX inference done in {onnx_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: accuracy assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "analysis_main(paths[\"onnx\"], paths[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plot\n",
    "plot_example(\n",
    "    paths[\"onnx\"], paths[\"target\"], outdir=folders[\"onnx_plot\"], with_graphics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "Batch_size vs inference time plot (PyTorch vs Onnx)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e80351b5c4fff56b32463330d4c3134db2ccc1cad158f50be317425bad0b8b08"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
